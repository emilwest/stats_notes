---
author: "Emil Westin"
format: html
editor: source
execute: 
  eval: true
  message: false
  warning: false
---

# Survey sampling

Survey sampling is a statistical process (statistisk unders√∂kning) characterized by the following:

-   A well-defined finite set of elements, a population. It must be clear which elements are a part of the population you wish to study.

-   The elements are measurable and there is a need for information about some population parameters.

-   A sampling frame (urvalsram) is needed, i.e. an operational representation of the elements. For example a hierarchy of sampling units that associate each element with a sampling unit.

-   A sample (stickprov) of elements is done by sampling sampling units from the sampling frame accoridning to some sample design (urvalsdesign), such that each element has a known positive probability of being selected. An exception being a total survey (totalunders√∂kning) where all elements are selected.

-   The sampled elements are observed (measured) with regards to the set of variables that are of interest for the survey accordning to a measurement plan.

-   The collected observations are finally used to estimate population parameters (total, mean, percent, quotients, regression coefficients, ...) where information is needed.

## Notation

Typically, the set notation is used in survey sampling. The variable (unders√∂kningsvariabeln) is denoted as $y$ instead of the traditional $x$.

$\bar{y} = \frac{1}{n}\sum _{i \in s}y _i, \text{ where }s= {1,2,...,n}$

The population is

$U = {1,2,...,k,...,N}$

Let $y _k$ denote the value of element $k$ in the population.

The parameter (m√•lstorhet) $t$ (the total of $y$) is very important.

$t = t _y = \sum _{k \in U} y _k = \sum _{U} y _k = y _1 + y _2 + ... + y _N$

The mean is a function of the total:

$\bar{y} _U = \frac{\sum _U y _k}{N} = \frac{\sum _U y _k}{\sum _U 1} = \frac{\sum _U y _k}{\sum _U z _k} = \frac{t _y}{t _z}, \text{ where } z _k = 1$

To estimate the parameters, a sample $s$ is drawn of size $n$ according to a sampling design $p(s)$. In some sampling designs, $N$ is known and can be trated as a constant. In others, $N$ must be estimated.

## Sampling design

### Simple random sample (OSU - obundet slumpm√§ssigt urval)

Without replacement, ie an object can only be included in a sample once. The definition of OSU is that each possible combination of samples drawn of (fixed) size $n$ from $N$ are equally likely (have the same probability of being drawn).

$p(s) = \frac{1}{\binom{N}{n}}$

For example, consider the objects A, B, C, D, E ($N=5$). If we select $n=3$ objects, this can be done in $\binom{5}{3} = \frac{5!}{3!(5-3)!} = 10$ different ways. The probability for each of the possible samples is $p(s) = 1/10=0.1$. The sum of all $p(s)=1$ ie $\sum _{\text{all s}} p(s) = 1$.

```{r}
#| echo: false
#| eval: true
library(dplyr)
df <- tibble(
  Nr = 1:10, 
  A = c("A","A","A","A","A","A", "","","",""),
  B= c("B","B","B","","","", "B","B","B",""), 
  C = c("C","","","C","C","", "C","C","","C"), 
  D= c("","D","","D","","D", "D","","D","D"), 
  E = c("","","E","","E","E", "","E","E","E"), 
  `p(s)` = rep(0.1,10)
)
df
```

Note for example that the probability of selecting A is $n/N=6/10 = 0.6$. The same probability for B, C, D and E.

#### First order inclusion probability

Inklusionssannolikhet (f√∂rsta ordningens).

The probability of selecting element $k$ is

$P(k \in s) = \pi _k = \sum p(s)$ for all $s$ where $k$ is inkluded.

In the example above for $N=5,n=3$, $\pi _k = n/N = 0.6$ (for OSU).

This can be shown by asking: what is the probability of an event happening k times in n trials, without replacement? We can use the hypergeometric distribution.

$P(k \in s) = \frac{\binom{1}{1}\binom{N-1}{n-1}}{\binom{N}{n}} = \frac{\frac{(N-1)!}{(n-1)![N-1-(n-1)]!}}{\frac{N!}{n!(N-n)!}} = \frac{(N-1)!n!(N-n)!}{N!(n-1)!(N-n)!} = \frac{n}{N}$

An indicator variable can be used to indicate if element k is included in the sample: $I _k = 1$ if $k\in s$ and $I _k = 0$ if $k \notin s$. $I _k$ is a Bernoulli random variable.

$E(I _k) = P(I _k = 1)\times 1 + P(I _k = 0)\times 0 = P(I _k = 1) = \pi _k$

Also note that:

$E(I _k^2) = P(I _k^2 = 1)\times 1 + P(I _k^2 = 0)\times 0 = P(I _k^2 = 1) = \pi _k$

since $I _k^2 = 1^2 = 1$ if $k\in s$.

The variance of the inclusion probability is

$$
V(I _k) = E[I _k - E(I _k)]^2 \\
= E[I _k^2] - [E(I _k)]^2 \\
= E[I _k] - [E(I _k)]^2 \\
= \pi _k-\pi _k^2  \\
= \pi _k(1-\pi _k)
$$

--

#### Second order inclusion probability

The probability of selecting both element $k$ and $l$ is

$P(k \cap l \in s) = P(I _k I _l = 1) = \sum p(s)$ for all s where k and l are included. Note that $I _k I _l = 1$ if and only if both k and l are in the sample.

$E(I _k I _l ) = P(I _k I _l = 1) = \pi _{kl}$

which is

$$
\begin{align}
\pi _{kl} &= P(k \cap l \in s)  \\ 
&= \frac{\binom{2}{2}\binom{N-2}{n-2}}{\binom{N}{n}} \\
&= \frac{\frac{(N-2)!}{(n-2)![N-2-(n-2)]!}}{\frac{N!}{n!(N-n)!}} \\
&= \frac{(N-2)!n!(N-n)!}{N!(n-2)!(N-n)!} \\
&=  \frac{n(n-1)}{N(N-1)}
\end{align}
$$

For example, from the example table we can visually see that selecting both A and B has a probability of 0.3 ($\sum p(s) = 0.1+0.1+0.1$).

```{r}
#| echo: false
#| eval: true
df
```

which is shown to be equal to

$\pi _{kl} = \frac{3(2-1)}{5(5-1)} = \frac{6}{20} = \frac{3}{10} = 0.3$

## Horvitz-Thompson Estimator

The HT-estimator is a general unbiased estimator regardless of the sampling design.

$$
\hat{t} = \hat{t} _y = \hat{t} _{yHT} = \sum _{k \in s} \frac{y _k}{\pi _k} = \sum _{k \in s}d _k y _k = \frac{N}{n} \sum _{k \in s} y _k = N\bar{y} _s
$$

where $d _k = 1/\pi _k = N/n$ (OSU) is the design weight (designvikt) and is often added as a variable to the dataset.

The design weight can be interpreted as how many objects a certain object from the sample shall represent. If $N=1000, n=100$, then

$\pi _k = n/N = 100/1000 = 0.1$

$d _k = N/n = 1000/100 = 10$

For example, for a sample of 100 schools from a population of 1000 schools, each school in the sample represents 10 schools in the population.

$E(\hat{t}) = E(\sum _{k \in s} \frac{y _k}{\pi _k}) = E(\sum _{k \in U} I _k \frac{y _k}{\pi _k}) = \sum _{k \in U} E(I _k) \frac{y _k}{\pi _k} = \sum _{k \in U} \pi _k \frac{y _k}{\pi _k} = \sum _{k \in U} y _k = t$

Note that:

-   Indcator variable $I _k$ was added so sum over s changed to sum over U

-   $y$ is regarded as a fixed constant in this type of survey sampling. The randomness is instead introduced from the sampling design. The advantage is that no assumptions are needed (in the case of full response).

Note that the mean is estimated with

$$
\hat{\bar{y}} _U = \frac{\sum _s y _k/\pi _k}{\sum _s d _k} = \frac{\hat{t} _{HT}}{\hat{N}} = \frac{\sum _{k \in s} (N/n)y _k}{N} = \frac{1}{n} \sum _{k \in s} y _k = \bar{y} _s
$$

In OSU, $\hat{N} = \sum _s d _k = \sum _s N/n = N/n \sum _s 1 = N$.

$E(\hat{\bar{y}} _U) = \frac{E(\sum _s y _k/\pi _k)}{N} = \frac{\sum _U y _k}{N} = \frac{t}{N}$

### Variance of the Horvitz-Thompson Estimator

Variance of a linear combination of variables general formula:

$$
\begin{align}
Var(a _1x _1 + \dots a _nx _n) &= \sum _{i=1}^n a _i^2 V(x _i) + 2 \sum _{i=1}^n \sum _{j>i}^n a _i a _j \text{ Cov}(x _i,x _j) \\
&= \sum _{i=1}^n a _i^2 V(x _i) +  \sum _{i=1}^n \sum _{j=1}^n a _i a _j \text{ Cov}(x _i,x _j), i\neq j
\end{align}
$$

$$
\begin{align}
V(\hat{t}) &= V(\sum _{k \in s}\frac{y _k}{\pi _k}) \\
&=  V(\sum _{k \in U} I _k \frac{y _k}{\pi _k}) \\
&= \sum _{k \in U} (\frac{y _k}{\pi _k})^2 V(I _k) +  \sum _{k \in U} \sum _{l \in U} \frac{y _k}{\pi _k} \frac{y _l}{\pi _l} \text{ Cov}(I _k,I _l), k\neq l
\end{align}
$$

Where the variance of the inclusion indicator variable is

$$V(I _k) = E[I _k - E(I _k)]^2 = E[I _k^2] - [E(I _k)]^2 = E[I _k] - [E(I _k)]^2 \\
= \pi _k-\pi _k^2 = \pi _k(1-\pi _k)$$

and the covariance

$\text{ Cov}(I _k,I _l) = E[I _k I _l] - E[I _k]E[I _l] = \pi _{kl}-\pi _k \pi _l$

Also note that $\pi _{kk} = \pi _{k}$

Collect expressions

$$
\begin{align}
V(\hat{t}) &= \sum _{k \in U} (\frac{y _k}{\pi _k})^2 V(I _k) +  \sum _{k \in U} \sum _{l \in U} \frac{y _k}{\pi _k} \frac{y _l}{\pi _l} \text{ Cov}(I _k,I _l), k\neq l \\
&=  \sum _{k \in U} (\frac{y _k}{\pi _k})^2 \pi _k(1-\pi _k) +  \sum _{k \in U} \sum _{l \in U} \frac{y _k}{\pi _k} \frac{y _l}{\pi _l} (\pi _{kl}-\pi _k \pi _l), k\neq l \\
&= \sum _{k \in U} \frac{y _k}{\pi _k}\frac{y _k}{\pi _k} (\pi _{kk}-\pi _k\pi _k) + \sum _{k \in U} \sum _{l \in U} \frac{y _k}{\pi _k} \frac{y _l}{\pi _l} (\pi _{kl}-\pi _k \pi _l), k\neq l \\
&= \sum _{k \in U} \sum _{l \in U} \frac{y _k}{\pi _k} \frac{y _l}{\pi _l} (\pi _{kl}-\pi _k \pi _l) \\
&= \sum _{k \in U} \sum _{l \in U} (\pi _{kl}-\pi _k \pi _l)  \frac{y _k}{\pi _k} \frac{y _l}{\pi _l}
\end{align}
$$

Notice that the two sums could be simplified to a single sum because when $k=l$ the first term has the same form as the second term. Imagine that the first term was the diagonal in a covariance matrix and the second term was on the off-diagonals.

It can be shown that

$V(\hat{t}) = \sum _{k \in U} \sum _{l \in U} (\pi _{kl}-\pi _k \pi _l) \frac{y _k}{\pi _k} \frac{y _l}{\pi _l} = N^2\frac{1-\frac{n}{N}}{n} S^2 _{yU}$

where

$$
\begin{align} 
S^2 _{yU} &= \frac{1}{N-1}\sum _{k \in U} (y _k - \bar{y} _U)^2 \\
&= \frac{1}{N} (\sum _{k \in U} y^2 _k - \frac{1}{N-1}\sum _{k \in U}\sum _{l \in U} y _ky _l), k \neq l
\end{align}
$$

Proof:

$$
\begin{align} 
V(\hat{t}) &= \sum _{k \in U} \sum _{l \in U} (\pi _{kl}-\pi _k \pi _l)  \frac{y _k}{\pi _k} \frac{y _l}{\pi _l} \\
&= \sum _{k \in U} \frac{y _k}{\pi _k}\frac{y _k}{\pi _k} (\pi _{kk}-\pi _k\pi _k) + \sum _{k \in U, \\ k \neq l} \sum _{l \in U} \frac{y _k}{\pi _k} \frac{y _l}{\pi _l} (\pi _{kl}-\pi _k \pi _l) \\
&= \sum _{k \in U} \frac{y _k}{\frac{n}{N}}\frac{y _k}{\frac{n}{N}} (\frac{n}{N}-\frac{n}{N}\frac{n}{N}) +  \sum _{k \in U, \\ k \neq l} \sum _{l \in U}  (\frac{n(n-1)}{N(N-1)}-\frac{n}{N} \frac{n}{N}) \frac{y _k}{\frac{n}{N}} \frac{y _l}{\frac{n}{N}} \\
&= \sum _{k \in U}  \frac{n}{N}(1-\frac{n}{N}) \frac{N^2}{n^2} y _k^2 + \sum _{k \in U, \\ k \neq l} \sum _{l \in U} \frac{n}{N}(\frac{n-1}{N-1}-\frac{n}{N})\frac{N^2}{n^2}y _k y _l \\
&=  \frac{N^2}{n^2}(1 - \frac{n}{N})\frac{n}{N} (\sum _{k \in U} y _k^2 + \sum _{k \in U, \\ k \neq l} \sum _{l \in U} \frac{1}{(1 - \frac{n}{N})}(\frac{n-1}{N-1}-\frac{n}{N}) y _k y _l) \\
\end{align}
$$

Where 

$\pi _{k} = \frac{n}{N}$

and

$\pi _{kl} = P(k \cap l \in s) = \frac{\binom{2}{2}{\binom{N-2}{n-2}}}{\binom{N}{n}} = \frac{n(n-1)}{N(N-1)}, k \neq l$.

Simplify expression outside the parenthesis

$$
\begin{align} 
&= N^2 \frac{(1 - \frac{n}{N})}{n}\frac{1}{N} (\sum _{k \in U} y _k^2 + \sum _{k \in U, \\ k \neq l} \sum _{l \in U} \frac{1}{(1 - \frac{n}{N})}(\frac{n-1}{N-1}-\frac{n}{N}) y _k y _l) \\
\end{align}
$$

Note that the right term inside the parenthesis can be simplified as follows: $\frac{1}{(1 - \frac{n}{N})}(\frac{n-1}{N-1}-\frac{n}{N}) = \frac{1}{(\frac{N-n}{N})}(\frac{N(n-1)-n(N-1)}{N(N-1)}) = \frac{1}{(\frac{N-n}{N})}(\frac{n-N}{N(N-1)}) = \frac{N}{N-n} (\frac{n-N}{N(N-1)}) = \frac{1}{N-1}$

It can now be simplified as following:

$$
\begin{align} 
&= N^2 \frac{(1 - \frac{n}{N})}{n}\frac{1}{N} (\sum _{k \in U} y _k^2 + \frac{1}{N-1} \sum _{k \in U, \\ k \neq l} \sum _{l \in U}  y _k y _l) \\
&= N^2 \frac{(1 - \frac{n}{N})}{n} S^2 _{yU}
\end{align}
$$



## Dragningsschema (Urvalsschema)

An algorithm for performing sampling (urvalsdragning). The goal of the algorithm is to fulfill the sampling design.

-   Dragsekventiella: En serie slumpm√§ssiga f√∂rs√∂k fr√•n hela populationen. Ex Bingolotto med tombola med numren 1 till 37.

-   Listsekventiella: Urvalsramens enheter g√•s igenom enhet f√∂r enhet. Ett slumpm√§ssigt f√∂rs√∂k utf√∂rs f√∂r varje enhet f√∂r att avg√∂ra om enheten ska ing√• i urvalet eller inte. (most common)

### Sunter

For OSU, the most common algorithm is Sunter (1977). The algorithm simply assigns a random number drawn from uniform distribution (0,1) as a key to each item, then sorts all items using the key and selects the smallest k items.

Here is an implementation in R/tidyverse:

```{r}
#| echo: true
#| eval: true
#| warning: false 

library(tidyverse)
library(survey)
data(api)

k <- 3
# if you have stratum, you can also create a column 'small n'/'nh' with the number of observation to select from each stratum and use that instead of k

selection <- apistrat %>% 
  mutate(unif = runif(n = n())) %>% 
  # sorterar data on stratum (stype) and uniform random variable value
  arrange(stype, unif) %>% 
  group_by(stype) %>%
  mutate(x = sequence(n())) %>%
  # vi v√§ljer ut nh f√∂rsta raderna per stratum
  filter(x <= k) %>%
  ungroup()


selection %>% select(1:4, enroll, unif, x)

```

### Bernoulli sampling

Elements in the frame (ramen) $k=1,2,...,N$. Let $\pi$ be a fixed constant $0 < \pi < 1$.

Let $\varepsilon_1,\varepsilon_2, ..., \varepsilon_N$ be $N$ independent generated random variables from a uniform distribution in the interval (0,1).

Schema: element $k$ is chosen if $\varepsilon_k < \pi$.

The sample size is random. The expected sample size it $N \times \pi$.

### Systematic sampling

Systematiskt urval. This design is not measurable because the variance cannot be estimated.

Let ùëé be sampling interval

Let ùëõ be the integer part by dividing ùëÅ/ùëé and let ùëê be the rest

Then, ùëÅ=ùëõùëé+ùëê

1)  200=20√ó10+0

2)  201=20√ó10+1

3)  207=20√ó10+7

If rest ùëê\>0 the sample will be ùëõ or ùëõ+1

In 3) above, ùëõ=21 with random start between 1 and 7 In 3) above, ùëõ=20 with random start between 8 and 10

This sampling design can be useful for sampling in a geographical area, for example a grid of 1x1 meter squares are the objects. Compared to OSU the sampling may be more evenly distributed, whereas OSU can sample many observations in a certain location.

### Poisson sampling

If $x_k$ is a help variable correlated with $y_k$, then it's possible to choose

$\pi _k = x _k / t _x, n x _k < t _x$

A list sequential schema is given by:

Let $\varepsilon_1,\varepsilon_2, ..., \varepsilon_N$ be $N$ independent generated random variables from a uniform distribution in the interval (0,1). Element $k$ is chosen if $\varepsilon_k < \pi$.

### Probability Proportional to Size sampling

-   pps (with replacement)

-   $\pi$ps (without replacement)

## Calibration

```{r}
#| eval: false

library(devtools)
devtools::install_github(repo = "DiegoZardetto/ReGenesees")
library(ReGenesees)

# skapa designinformation
design4 <- e.svydesign(data = as.data.frame(data),
                        ids = ~id,
                        strata = ~stratum,
                        weights = ~designvikt,
                        fpc = ~StoraN
)
design4


#utb  3535715 2654451 1488358

# skapa en population template, det beh√∂vs f√∂r kalibrering
aux4 <- pop.template(design4, calmodel=~(aldkon+utb-1), partition = F)
aux4
aux4[1] <- 841269
aux4[2] <- 950487
aux4[3] <- 1236159
aux4[4] <- 830221
aux4[5] <- 796725
aux4[6] <- 911748
aux4[7] <- 1211723
aux4[8] <- 900192
aux4[9] <- 2654451 # OBS! utb kategori 2
aux4[10] <- 1488358 # OBS! utb kategori 3

aux4

# kalibrering utf√∂rs h√§r
descal4 <- e.calibrate(design = design4,
                        df.population = aux4,
                        calmodel= ~(aldkon+utb-1),
                        partition = FALSE,
                        calfun = "linear",
                        bounds = c(-100, 100),
                        aggregate.stage = NULL
)

summary(descal1b)
weights(descal1b) %>% unique
data$ald

# Estimationsanrop
svystatTM(descal4, ~y, by = ~ald)
svystatTM(descal4, ~y, by = ~ald, estimator = c("Mean"))



```
